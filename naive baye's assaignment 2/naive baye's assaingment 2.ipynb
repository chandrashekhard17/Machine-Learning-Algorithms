{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "880fbf46",
   "metadata": {},
   "source": [
    "### Q1. Probability that an employee is a smoker given they use the health insurance plan\n",
    "\n",
    "We are given:\n",
    "- 70% of employees use the health insurance plan, \\( P(Plan) = 0.7 \\).\n",
    "- 40% of the employees who use the plan are smokers, \\( P(Smoker|Plan) = 0.4 \\).\n",
    "\n",
    "The question asks for \\( P(Smoker|Plan) \\), which is directly provided as **40%** or **0.4**. \n",
    "\n",
    "This probability means that given an employee uses the company's health insurance plan, there is a 40% chance that the employee is a smoker.\n",
    "\n",
    "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "**Bernoulli Naive Bayes** and **Multinomial Naive Bayes** differ primarily in how they handle the feature space:\n",
    "\n",
    "- **Bernoulli Naive Bayes**:\n",
    "  - This classifier is used when the features are binary (0 or 1), meaning it expects the presence or absence of features.\n",
    "  - It is suitable for problems like text classification, where the model checks whether a particular word is present in a document or not.\n",
    "  - It is ideal for cases where features represent binary outcomes, such as whether a word appears in an email.\n",
    "\n",
    "- **Multinomial Naive Bayes**:\n",
    "  - This classifier is used for discrete (count-based) features. It is typically applied in text classification tasks where the frequency of words (count data) is the feature.\n",
    "  - It assumes that features represent counts of occurrences, making it appropriate when the features are not binary but reflect the number of times a feature occurs (e.g., word frequency in a document).\n",
    "\n",
    "Thus, **Bernoulli Naive Bayes** is suited for binary feature vectors, while **Multinomial Naive Bayes** handles frequency-based feature vectors.\n",
    "\n",
    "### Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Bernoulli Naive Bayes assumes binary features and does not inherently handle missing values. When faced with missing data in real-world scenarios:\n",
    "- One option is to fill or impute the missing values with 0 or 1 based on the most likely occurrence (for instance, filling missing words as \"absent\").\n",
    "- Another approach is to use imputation techniques (mean/mode imputation) to replace missing data, though this assumes some prior knowledge about how missingness relates to the problem.\n",
    "\n",
    "Handling missing values effectively in Bernoulli Naive Bayes requires pre-processing, as the model does not handle missing data natively.\n",
    "\n",
    "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Yes, **Gaussian Naive Bayes** can be used for **multi-class classification**. The Gaussian Naive Bayes model is versatile and can classify data into multiple categories by calculating the likelihood of each class and predicting the class with the highest posterior probability. \n",
    "\n",
    "Gaussian Naive Bayes assumes that the continuous features in the dataset are normally distributed, and it is effective for multi-class problems where each class can have its own normal distribution for each feature.\n",
    "\n",
    "### Q5. Assignment\n",
    "\n",
    "#### Data Preparation:\n",
    "Download the **Spambase Data Set** from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains features extracted from email messages to predict whether an email is spam or not.\n",
    "\n",
    "#### Implementation:\n",
    "\n",
    "You will implement three Naive Bayes classifiers:\n",
    "- **Bernoulli Naive Bayes**\n",
    "- **Multinomial Naive Bayes**\n",
    "- **Gaussian Naive Bayes**\n",
    "\n",
    "Hereâ€™s how to implement these classifiers using **scikit-learn** in Python:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('spambase.data', header=None)\n",
    "X = data.iloc[:, :-1]  # Features\n",
    "y = data.iloc[:, -1]   # Target (spam or not spam)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# 10-fold cross-validation\n",
    "for model, name in [(bernoulli_nb, \"Bernoulli\"), (multinomial_nb, \"Multinomial\"), (gaussian_nb, \"Gaussian\")]:\n",
    "    print(f\"\\n{name} Naive Bayes:\")\n",
    "    accuracy = cross_val_score(model, X_train, y_train, cv=10, scoring='accuracy').mean()\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Fit models\n",
    "bernoulli_nb.fit(X_train, y_train)\n",
    "multinomial_nb.fit(X_train, y_train)\n",
    "gaussian_nb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bernoulli = bernoulli_nb.predict(X_test)\n",
    "y_pred_multinomial = multinomial_nb.predict(X_test)\n",
    "y_pred_gaussian = gaussian_nb.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "def print_metrics(y_test, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"\\n{model_name} Naive Bayes:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Metrics for each classifier\n",
    "print_metrics(y_test, y_pred_bernoulli, \"Bernoulli\")\n",
    "print_metrics(y_test, y_pred_multinomial, \"Multinomial\")\n",
    "print_metrics(y_test, y_pred_gaussian, \"Gaussian\")\n",
    "```\n",
    "\n",
    "#### Results:\n",
    "For each classifier, you will report:\n",
    "- **Accuracy**: Measures how often the classifier is correct.\n",
    "- **Precision**: The ratio of true positives to the sum of true positives and false positives.\n",
    "- **Recall**: The ratio of true positives to the sum of true positives and false negatives.\n",
    "- **F1 Score**: The harmonic mean of precision and recall, balancing the two.\n",
    "\n",
    "#### Discussion:\n",
    "Based on the results from the above code, you would observe that:\n",
    "- **Multinomial Naive Bayes** often performs best on text classification problems like this because it is suited for count-based features (like word frequencies in emails).\n",
    "- **Bernoulli Naive Bayes** might perform well if you convert the features to binary, i.e., presence or absence of certain words.\n",
    "- **Gaussian Naive Bayes** might underperform because it assumes normally distributed features, which is rarely the case with text data.\n",
    "\n",
    "One limitation of Naive Bayes you might observe is its assumption of feature independence, which may not hold in real-world datasets, leading to suboptimal performance in some cases.\n",
    "\n",
    "#### Conclusion:\n",
    "The **Multinomial Naive Bayes** classifier is expected to perform best due to its suitability for handling count-based features like word frequencies in emails. For future work, you could explore ways to handle feature dependence (such as using more complex models like logistic regression or support vector machines) and apply feature engineering techniques to improve performance further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09aa13e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1549b9f7",
   "metadata": {},
   "source": [
    "### Q1. What is Bayes' theorem?\n",
    "\n",
    "Bayes' theorem is a mathematical formula used to determine conditional probabilities. Named after the statistician Thomas Bayes, it describes how to update the probability of a hypothesis based on new evidence. This theorem is foundational in the field of probability theory and plays a crucial role in many machine learning algorithms, especially in classification tasks.\n",
    "\n",
    "In essence, Bayes' theorem helps you revise predictions or probabilities when new information or data is available. For instance, it can be used to predict whether a person has a particular disease based on test results, or to classify emails as spam or not based on their content. The theorem works by inverting the relationship between two events to compute the likelihood of one event given the occurrence of another.\n",
    "\n",
    "### Q2. What is the formula for Bayes' theorem?\n",
    "\n",
    "The formula for Bayes' theorem is expressed as:\n",
    "\n",
    "\\[\n",
    "P(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( P(H|E) \\) is the **posterior probability**, or the probability of hypothesis \\( H \\) given evidence \\( E \\).\n",
    "- \\( P(E|H) \\) is the **likelihood**, or the probability of observing evidence \\( E \\) given that \\( H \\) is true.\n",
    "- \\( P(H) \\) is the **prior probability** of the hypothesis, which reflects what we believed before seeing the evidence.\n",
    "- \\( P(E) \\) is the **marginal likelihood**, or the probability of the evidence under all possible hypotheses.\n",
    "\n",
    "This formula essentially tells us how to update the probability of a hypothesis \\( H \\) after we have seen some evidence \\( E \\).\n",
    "\n",
    "### Q3. How is Bayes' theorem used in practice?\n",
    "\n",
    "Bayes' theorem is used in a wide range of fields, from medicine to machine learning to natural language processing. Below are a few practical applications:\n",
    "\n",
    "- **Medical Diagnosis:** Suppose you are trying to diagnose a disease based on test results. The prior probability \\( P(H) \\) might represent how common the disease is in the population, while \\( P(E|H) \\) could represent the probability of testing positive given that the patient has the disease. Bayes' theorem helps you calculate the probability that the patient has the disease given that they tested positive.\n",
    "\n",
    "- **Spam Filtering:** In email classification, the hypothesis \\( H \\) might be whether an email is spam or not, and the evidence \\( E \\) could be the presence of specific words in the email. Using Bayes' theorem, spam filters calculate the probability that an email is spam based on the words it contains.\n",
    "\n",
    "- **Machine Learning (Naive Bayes Classifier):** Naive Bayes, a machine learning algorithm, is based on Bayes' theorem. It's \"naive\" because it assumes that the features (or predictors) are independent of each other, which is rarely true in real-world data, but the algorithm works well in many practical cases.\n",
    "\n",
    "- **Stock Market Prediction:** Investors use Bayes' theorem to update the likelihood of market trends or stock behavior based on new economic indicators or news.\n",
    "\n",
    "In these and many other cases, Bayes' theorem helps to improve decision-making by incorporating new data into existing models or beliefs.\n",
    "\n",
    "### Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "\n",
    "Bayes' theorem is a direct application of conditional probability. Conditional probability refers to the probability of an event occurring given that another event has occurred. The formula for conditional probability is:\n",
    "\n",
    "\\[\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "\\]\n",
    "\n",
    "This gives the probability of event \\( A \\) occurring given that event \\( B \\) has occurred.\n",
    "\n",
    "Bayes' theorem builds upon this idea by providing a way to reverse the conditional probability. If you know \\( P(A|B) \\), Bayes' theorem allows you to compute \\( P(B|A) \\), which may not be directly obvious from the initial information.\n",
    "\n",
    "The relationship between the two is that Bayes' theorem takes conditional probabilities one step further by incorporating prior beliefs and likelihoods to calculate posterior probabilities. Essentially, conditional probability is a component of Bayes' theorem.\n",
    "\n",
    "### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "\n",
    "There are different types of Naive Bayes classifiers, and choosing the right one depends on the nature of the features in your dataset:\n",
    "\n",
    "1. **Gaussian Naive Bayes**: \n",
    "   - This is used when the features are continuous and are assumed to follow a normal (Gaussian) distribution.\n",
    "   - It’s useful when you’re working with data that is numerical and where the assumption of normal distribution holds, like in medical diagnostics or sensor data analysis.\n",
    "\n",
    "2. **Multinomial Naive Bayes**: \n",
    "   - This is used for discrete features and is commonly applied to text classification problems such as spam detection or document categorization.\n",
    "   - It assumes that each feature (e.g., word) follows a multinomial distribution, and it works well when your data is count-based, like word frequency counts.\n",
    "\n",
    "3. **Bernoulli Naive Bayes**: \n",
    "   - This is also used for discrete features, but it works with binary data (i.e., 0s and 1s).\n",
    "   - It is most suitable for binary feature sets, where the features can only have two possible outcomes (e.g., word presence/absence in text).\n",
    "\n",
    "To decide which Naive Bayes classifier to use, you need to consider:\n",
    "- **The type of features**: Continuous data leads to Gaussian, count-based data leads to Multinomial, and binary data to Bernoulli.\n",
    "- **The nature of the problem**: If it’s a text classification task (e.g., sentiment analysis or spam filtering), Multinomial Naive Bayes often works well.\n",
    "\n",
    "### Q6. Assignment: Naive Bayes classification task\n",
    "\n",
    "You have a dataset with two features, \\( X1 \\) and \\( X2 \\), and two possible classes, \\( A \\) and \\( B \\). You want to classify a new instance with \\( X1 = 3 \\) and \\( X2 = 4 \\).\n",
    "\n",
    "#### Data summary (Frequency counts for each feature value per class):\n",
    "| Class | \\( X1 = 1 \\) | \\( X1 = 2 \\) | \\( X1 = 3 \\) | \\( X2 = 1 \\) | \\( X2 = 2 \\) | \\( X2 = 3 \\) | \\( X2 = 4 \\) |\n",
    "|-------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n",
    "| A     | 3            | 3            | 4            | 4            | 3            | 3            | 3            |\n",
    "| B     | 2            | 2            | 1            | 2            | 2            | 2            | 3            |\n",
    "\n",
    "#### Step-by-Step Solution:\n",
    "\n",
    "1. **Step 1: Calculate Prior Probabilities**  \n",
    "Since we are assuming equal prior probabilities for both classes \\( A \\) and \\( B \\):\n",
    "\n",
    "\\[\n",
    "P(A) = P(B) = 0.5\n",
    "\\]\n",
    "\n",
    "2. **Step 2: Calculate Likelihoods**  \n",
    "For class \\( A \\):\n",
    "- \\( P(X1 = 3 | A) = \\frac{4}{13} \\)\n",
    "- \\( P(X2 = 4 | A) = \\frac{3}{13} \\)\n",
    "\n",
    "For class \\( B \\):\n",
    "- \\( P(X1 = 3 | B) = \\frac{1}{8} \\)\n",
    "- \\( P(X2 = 4 | B) = \\frac{3}{8} \\)\n",
    "\n",
    "3. **Step 3: Apply Bayes' theorem to calculate the posterior for both classes**  \n",
    "For class \\( A \\):\n",
    "\n",
    "\\[\n",
    "P(A | X1 = 3, X2 = 4) = P(X1 = 3 | A) \\times P(X2 = 4 | A) \\times P(A) = \\frac{4}{13} \\times \\frac{3}{13} \\times 0.5\n",
    "\\]\n",
    "\n",
    "For class \\( B \\):\n",
    "\n",
    "\\[\n",
    "P(B | X1 = 3, X2 = 4) = P(X1 = 3 | B) \\times P(X2 = 4 | B) \\times P(B) = \\frac{1}{8} \\times \\frac{3}{8} \\times 0.5\n",
    "\\]\n",
    "\n",
    "4. **Step 4: Compare the posterior probabilities**  \n",
    "- \\( P(A | X1 = 3, X2 = 4) = \\frac{12}{13^2} \\times 0.5 \\)\n",
    "- \\( P(B | X1 = 3, X2 = 4) = \\frac{3}{64} \\times 0.5 \\)\n",
    "\n",
    "Since \\( P(A | X1 = 3, X2 = 4) \\) is greater than \\( P(B | X1 = 3, X2 = 4) \\), the Naive Bayes classifier would predict **Class A** for the new instance.\n",
    "\n",
    "### Conclusion:\n",
    "The Naive Bayes classifier would classify the instance with \\( X1 = 3 \\) and \\( X2 = 4 \\) as belonging to **Class A**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f104f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
